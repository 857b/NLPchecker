{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/.switch/dl/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CamembertForMaskedLM, CamembertTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PreTrainedTokenizer in transformers:\n",
      "\n",
      "transformers.PreTrainedTokenizer = class PreTrainedTokenizer(transformers.tokenization_utils_base.PreTrainedTokenizerBase)\n",
      " |  transformers.PreTrainedTokenizer(**kwargs)\n",
      " |  \n",
      " |  Base class for all slow tokenizers.\n",
      " |  \n",
      " |  Inherits from :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase`.\n",
      " |  \n",
      " |  Handle all the shared methods for tokenization and special tokens as well as methods downloading/caching/loading\n",
      " |  pretrained tokenizers as well as adding tokens to the vocabulary.\n",
      " |  \n",
      " |  This class also contain the added tokens in a unified way on top of all tokenizers so we don't have to handle the\n",
      " |  specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).\n",
      " |  \n",
      " |  Class attributes (overridden by derived classes)\n",
      " |  \n",
      " |      - **vocab_files_names** (:obj:`Dict[str, str]`) -- A dictionary with, as keys, the ``__init__`` keyword name of\n",
      " |        each vocabulary file required by the model, and as associated values, the filename for saving the associated\n",
      " |        file (string).\n",
      " |      - **pretrained_vocab_files_map** (:obj:`Dict[str, Dict[str, str]]`) -- A dictionary of dictionaries, with the\n",
      " |        high-level keys being the ``__init__`` keyword name of each vocabulary file required by the model, the\n",
      " |        low-level being the :obj:`short-cut-names` of the pretrained models with, as associated values, the\n",
      " |        :obj:`url` to the associated pretrained vocabulary file.\n",
      " |      - **max_model_input_sizes** (:obj:`Dict[str, Optinal[int]]`) -- A dictionary with, as keys, the\n",
      " |        :obj:`short-cut-names` of the pretrained models, and as associated values, the maximum length of the sequence\n",
      " |        inputs of this model, or :obj:`None` if the model has no maximum input size.\n",
      " |      - **pretrained_init_configuration** (:obj:`Dict[str, Dict[str, Any]]`) -- A dictionary with, as keys, the\n",
      " |        :obj:`short-cut-names` of the pretrained models, and as associated values, a dictionary of specific arguments\n",
      " |        to pass to the ``__init__`` method of the tokenizer class for this pretrained model when loading the\n",
      " |        tokenizer with the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`\n",
      " |        method.\n",
      " |      - **model_input_names** (:obj:`List[str]`) -- A list of inputs expected in the forward pass of the model.\n",
      " |      - **padding_side** (:obj:`str`) -- The default value for the side on which the model should have padding\n",
      " |        applied. Should be :obj:`'right'` or :obj:`'left'`.\n",
      " |  \n",
      " |  Args:\n",
      " |      model_max_length (:obj:`int`, `optional`):\n",
      " |          The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is\n",
      " |          loaded with :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`, this\n",
      " |          will be set to the value stored for the associated model in ``max_model_input_sizes`` (see above). If no\n",
      " |          value is provided, will default to VERY_LARGE_INTEGER (:obj:`int(1e30)`).\n",
      " |      padding_side: (:obj:`str`, `optional`):\n",
      " |          The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      " |          Default value is picked from the class attribute of the same name.\n",
      " |      model_input_names (:obj:`List[string]`, `optional`):\n",
      " |          The list of inputs accepted by the forward pass of the model (like :obj:`\"token_type_ids\"` or\n",
      " |          :obj:`\"attention_mask\"`). Default value is picked from the class attribute of the same name.\n",
      " |      bos_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
      " |          A special token representing the beginning of a sentence. Will be associated to ``self.bos_token`` and\n",
      " |          ``self.bos_token_id``.\n",
      " |      eos_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
      " |          A special token representing the end of a sentence. Will be associated to ``self.eos_token`` and\n",
      " |          ``self.eos_token_id``.\n",
      " |      unk_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
      " |          A special token representing an out-of-vocabulary token. Will be associated to ``self.unk_token`` and\n",
      " |          ``self.unk_token_id``.\n",
      " |      sep_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
      " |          A special token separating two different sentences in the same input (used by BERT for instance). Will be\n",
      " |          associated to ``self.sep_token`` and ``self.sep_token_id``.\n",
      " |      pad_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
      " |          A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by\n",
      " |          attention mechanisms or loss computation. Will be associated to ``self.pad_token`` and\n",
      " |          ``self.pad_token_id``.\n",
      " |      cls_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
      " |          A special token representing the class of the input (used by BERT for instance). Will be associated to\n",
      " |          ``self.cls_token`` and ``self.cls_token_id``.\n",
      " |      mask_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
      " |          A special token representing a masked token (used by masked-language modeling pretraining objectives, like\n",
      " |          BERT). Will be associated to ``self.mask_token`` and ``self.mask_token_id``.\n",
      " |      additional_special_tokens (tuple or list of :obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):\n",
      " |          A tuple or a list of additional special tokens. Add them here to ensure they won't be split by the\n",
      " |          tokenization process. Will be associated to ``self.additional_special_tokens`` and\n",
      " |          ``self.additional_special_tokens_ids``.\n",
      " |  .. automethod:: __call__\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PreTrainedTokenizer\n",
      " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
      " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Size of the full vocabulary with the added tokens.\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids: Union[int, List[int]], skip_special_tokens: bool = False) -> Union[str, List[str]]\n",
      " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n",
      " |      added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`int` or :obj:`List[int]`):\n",
      " |              The token id (or token ids) to convert to tokens.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str` or :obj:`List[str]`: The decoded token(s).\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]\n",
      " |      Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
      " |      vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (:obj:`str` or :obj:`List[str]`): One or several token(s) to convert to token id(s).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int` or :obj:`List[int]`: The token id or list of token ids.\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens: List[str]) -> str\n",
      " |      Converts a sequence of token ids in a single string. The most simple way to do it is ``\" \".join(tokens)`` but\n",
      " |      we often want to remove sub-word tokenization artifacts at the same time\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (:obj:`List[str]`): The token to join in a string.\n",
      " |      Return: The joined tokens.\n",
      " |  \n",
      " |  get_added_vocab(self) -> Dict[str, int]\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0: List, token_ids_1: Union[List, NoneType] = None, already_has_special_tokens: bool = False) -> List[int]\n",
      " |      Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of ids of the first sequence.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              List of ids of the second sequence.\n",
      " |          already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the token list is already formatted with special tokens for the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, pair: bool = False) -> int\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not\n",
      " |          put this inside your training loop.\n",
      " |      \n",
      " |      Args:\n",
      " |          pair (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
      " |              sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of special tokens added to sequences.\n",
      " |  \n",
      " |  prepare_for_tokenization(self, text: str, is_split_into_words: bool = False, **kwargs) -> Tuple[str, Dict[str, Any]]\n",
      " |      Performs any necessary transformations before tokenization.\n",
      " |      \n",
      " |      This method should pop the arguments from kwargs and return the remaining :obj:`kwargs` as well. We test the\n",
      " |      :obj:`kwargs` at the end of the encoding process to be sure all the arguments have been used.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`):\n",
      " |              The text to prepare.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the text has been pretokenized.\n",
      " |          kwargs:\n",
      " |              Keyword arguments to use for the tokenization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple[str, Dict[str, Any]]`: The prepared text and the unused kwargs.\n",
      " |  \n",
      " |  prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Union[List[str], NoneType] = None, max_length: Union[int, NoneType] = None, max_target_length: Union[int, NoneType] = None, padding: str = 'longest', return_tensors: str = 'None', truncation=True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepare a batch that can be passed directly to an instance of :class:`~transformers.AutoModelForSeq2SeqLM`.\n",
      " |      \n",
      " |      Args:\n",
      " |          src_texts: (:obj:`List[str]`):\n",
      " |              List of documents to summarize or source language texts.\n",
      " |          tgt_texts: (:obj:`List[str]`, `optional`):\n",
      " |              List of summaries or target language texts.\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts). If\n",
      " |              left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          max_target_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length of decoder inputs (target language texts or summaries). If left unset or\n",
      " |              set to :obj:`None`, this will use the max_length value.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`True`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          **kwargs:\n",
      " |              Additional keyword arguments passed along to :obj:`self.__call__`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
      " |          - **labels** -- List of token ids for tgt_texts\n",
      " |      \n",
      " |          The full set of keys ``[input_ids, attention_mask, labels]``, will only be returned if tgt_texts is passed.\n",
      " |          Otherwise, input_ids, attention_mask will be the only keys.\n",
      " |  \n",
      " |  tokenize(self, text: str, **kwargs) -> List[str]\n",
      " |      Converts a string in a sequence of tokens, using the tokenizer.\n",
      " |      \n",
      " |      Note that, unlike Fast tokenizers (instances of PreTrainedTokenizerFast), this method won't replace the unknown\n",
      " |      tokens with the `unk_token` yet (this is done in the `encode()` method)\n",
      " |      \n",
      " |      Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies\n",
      " |      (BPE/SentencePieces/WordPieces). Takes care of added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`):\n",
      " |              The sequence to be encoded.\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Passed along to the model-specific ``prepare_for_tokenization`` preprocessing method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of tokens.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  is_fast\n",
      " |  \n",
      " |  vocab_size\n",
      " |      :obj:`int`: Size of the base vocabulary (without the added tokens).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      " |      sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  batch_decode(self, sequences: Union[List[int], List[List[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> List[str]\n",
      " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences (:obj:`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of decoded sentences.\n",
      " |  \n",
      " |  batch_encode_plus(self, batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_text_or_text_pairs (:obj:`List[str]`, :obj:`List[Tuple[str, str]]`, :obj:`List[List[str]]`, :obj:`List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also :obj:`List[List[int]]`, :obj:`List[Tuple[List[int], List[int]]]`):\n",
      " |              Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
      " |              string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
      " |              details in ``encode_plus``).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) -> List[int]\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
      " |      adding special tokens.\n",
      " |      \n",
      " |      This implementation does not add special tokens and this method should be overridden in a subclass.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`): The first tokenized sequence.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`): The second tokenized sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: The model input with special tokens.\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) -> List[int]\n",
      " |      Create the token type IDs corresponding to the sequences passed. `What are token type IDs?\n",
      " |      <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |      Should be overridden in a subclass if the model has a special way of building those.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`): The first tokenized sequence.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`): The second tokenized sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: The token type ids.\n",
      " |  \n",
      " |  decode(self, token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> str\n",
      " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      " |      tokens and clean up tokenization spaces.\n",
      " |      \n",
      " |      Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids (:obj:`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The decoded sentence.\n",
      " |  \n",
      " |  encode(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, **kwargs) -> List[int]\n",
      " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
      " |              method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          **kwargs: Passed along to the `.tokenize()` method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`, :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`: The tokenized ids of the\n",
      " |          text.\n",
      " |  \n",
      " |  encode_plus(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
      " |              method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  get_vocab(self) -> Dict[str, int]\n",
      " |      Returns the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      :obj:`tokenizer.get_vocab()[token]` is equivalent to :obj:`tokenizer.convert_tokens_to_ids(token)` when\n",
      " |      :obj:`token` is in the vocab.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The vocabulary.\n",
      " |  \n",
      " |  pad(self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = True, max_length: Union[int, NoneType] = None, pad_to_multiple_of: Union[int, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, verbose: bool = True) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      " |      in the batch.\n",
      " |      \n",
      " |      Padding side (left/right) padding token ids are defined at the tokenizer level (with ``self.padding_side``,\n",
      " |      ``self.pad_token_id`` and ``self.pad_token_type_id``)\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          If the ``encoded_inputs`` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      " |          result will use the same type unless you provide a different tensor type with ``return_tensors``. In the\n",
      " |          case of PyTorch tensors, you will lose the specific device of your tensors however.\n",
      " |      \n",
      " |      Args:\n",
      " |          encoded_inputs (:class:`~transformers.BatchEncoding`, list of :class:`~transformers.BatchEncoding`, :obj:`Dict[str, List[int]]`, :obj:`Dict[str, List[List[int]]` or :obj:`List[Dict[str, List[int]]]`):\n",
      " |              Tokenized inputs. Can represent one input (:class:`~transformers.BatchEncoding` or :obj:`Dict[str,\n",
      " |              List[int]]`) or a batch of tokenized inputs (list of :class:`~transformers.BatchEncoding`, `Dict[str,\n",
      " |              List[List[int]]]` or `List[Dict[str, List[int]]]`) so you can use this method during preprocessing as\n",
      " |              well as in a PyTorch Dataloader collate function.\n",
      " |      \n",
      " |              Instead of :obj:`List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),\n",
      " |              see the note above for the return type.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
      " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      " |               index) among:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Maximum length of the returned list and optionally padding length (see above).\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value.\n",
      " |      \n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              >= 7.5 (Volta).\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |  \n",
      " |  prepare_for_model(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
      " |      adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
      " |      manages a moving window (with user defined stride) for overflowing tokens\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], legacy_format: bool = True, filename_prefix: Union[str, NoneType] = None) -> Tuple[str]\n",
      " |      Save the full tokenizer state.\n",
      " |      \n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the\n",
      " |      :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizer.from_pretrained` class method.\n",
      " |      \n",
      " |      .. Note::\n",
      " |          A \"fast\" tokenizer (instance of :class:`transformers.PreTrainedTokenizerFast`) saved with this method will\n",
      " |          not be possible to load back in a \"slow\" tokenizer, i.e. in a :class:`transformers.PreTrainedTokenizer`\n",
      " |          instance. It can only be loaded in a \"fast\" tokenizer, i.e. in a\n",
      " |          :class:`transformers.PreTrainedTokenizerFast` instance.\n",
      " |      \n",
      " |      .. Warning::\n",
      " |         This won't save modifications you may have applied to the tokenizer after the instantiation (for instance,\n",
      " |         modifying :obj:`tokenizer.do_lower_case` after creation).\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str` or :obj:`os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
      " |          legacy_format (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether to save the tokenizer in legacy format (default), i.e. with tokenizer specific vocabulary and a\n",
      " |              separate added_tokens files or in the unified JSON file format for the `tokenizers` library. It's only\n",
      " |              possible to save a Fast tokenizer in the unified JSON format and this format is incompatible with\n",
      " |              \"slow\" tokenizers (not powered by the `tokenizers` library).\n",
      " |          filename_prefix: (:obj:`str`, `optional`):\n",
      " |              A prefix to add to the names of the files saved by the tokenizer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of :obj:`str`: The files saved.\n",
      " |  \n",
      " |  save_vocabulary(self, save_directory: str, filename_prefix: Union[str, NoneType] = None) -> Tuple[str]\n",
      " |      Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
      " |      \n",
      " |      This method won't save the configuration and special token mappings of the tokenizer. Use\n",
      " |      :meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save the whole state of the tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str`):\n",
      " |              The directory in which to save the vocabulary.\n",
      " |          filename_prefix (:obj:`str`, `optional`):\n",
      " |              An optional prefix to add to the named of the saved files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple(str)`: Paths to the files saved.\n",
      " |  \n",
      " |  truncate_sequences(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first', stride: int = 0) -> Tuple[List[int], List[int], List[int]]\n",
      " |      Truncates a sequence pair in-place following the strategy.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          num_tokens_to_remove (:obj:`int`, `optional`, defaults to 0):\n",
      " |              Number of tokens to remove using the truncation strategy.\n",
      " |          truncation_strategy (:obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              The strategy to follow for truncation. Can be:\n",
      " |      \n",
      " |              * :obj:`'longest_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
      " |              sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple[List[int], List[int], List[int]]`: The truncated ``ids``, the truncated ``pair_ids`` and the\n",
      " |          list of overflowing tokens.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, **kwargs) from builtins.type\n",
      " |      Instantiate a :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase` (or a derived class) from\n",
      " |      a predefined tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |              - A string, the `model id` of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      " |                Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under a\n",
      " |                user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
      " |              - A path to a `directory` containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                using the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`\n",
      " |                method, e.g., ``./my_model_directory/``.\n",
      " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      " |                ``./my_model_directory/vocab.txt``.\n",
      " |          cache_dir (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      " |              exist.\n",
      " |          resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      " |              exists.\n",
      " |          proxies (:obj:`Dict[str, str], `optional`):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          use_auth_token (:obj:`str` or `bool`, `optional`):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      " |              generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
      " |          revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (:obj:`str`, `optional`):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      " |              facebook/rag-token-base), specify it here.\n",
      " |          inputs (additional positional arguments, `optional`):\n",
      " |              Will be passed along to the Tokenizer ``__init__`` method.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like\n",
      " |              ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``,\n",
      " |              ``mask_token``, ``additional_special_tokens``. See parameters in the ``__init__`` for more details.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # We can't instantiate directly the base class `PreTrainedTokenizerBase` so let's show our examples on a derived class: BertTokenizer\n",
      " |          # Download vocabulary from huggingface.co and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
      " |      \n",
      " |          # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
      " |      \n",
      " |          # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
      " |      \n",
      " |          # You can link tokens to special vocabulary when instantiating\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
      " |          # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |          # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |          assert tokenizer.unk_token == '<unk>'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string: str) -> str\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
      " |      \n",
      " |      Args:\n",
      " |          out_string (:obj:`str`): The text to clean up.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The cleaned-up string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  max_len_sentences_pair\n",
      " |      :obj:`int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
      " |  \n",
      " |  max_len_single_sentence\n",
      " |      :obj:`int`: The maximum length of a sentence that can be fed to the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __annotations__ = {'max_model_input_sizes': typing.Dict[str, typing.Un...\n",
      " |  \n",
      " |  max_model_input_sizes = {}\n",
      " |  \n",
      " |  model_input_names = ['token_type_ids', 'attention_mask']\n",
      " |  \n",
      " |  padding_side = 'right'\n",
      " |  \n",
      " |  pretrained_init_configuration = {}\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {}\n",
      " |  \n",
      " |  slow_tokenizer_class = None\n",
      " |  \n",
      " |  vocab_files_names = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, tokenizers.AddedToken]]) -> int\n",
      " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
      " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
      " |      current vocabulary).\n",
      " |      \n",
      " |      Using : obj:`add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - Special tokens are carefully handled by the tokenizer (they are never split).\n",
      " |      - You can easily refer to special tokens using tokenizer class attributes like :obj:`tokenizer.cls_token`. This\n",
      " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
      " |      :class:`~transformers.BertTokenizer` :obj:`cls_token` is already registered to be :obj`'[CLS]'` and XLM's one\n",
      " |      is also registered to be :obj:`'</s>'`).\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict (dictionary `str` to `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Keys should be in the list of predefined special attributes: [``bos_token``, ``eos_token``,\n",
      " |              ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n",
      " |              ``additional_special_tokens``].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
      " |              assign the index of the ``unk_token`` to them).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to add a new classification token to GPT-2\n",
      " |          tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
      " |          model = GPT2Model.from_pretrained('gpt2')\n",
      " |      \n",
      " |          special_tokens_dict = {'cls_token': '<CLS>'}\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |          # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |      \n",
      " |          assert tokenizer.cls_token == '<CLS>'\n",
      " |  \n",
      " |  add_tokens(self, new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
      " |      it with indices starting from length of the current vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens (:obj:`str`, :obj:`tokenizers.AddedToken` or a list of `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Tokens are only added if they are not already in the vocabulary. :obj:`tokenizers.AddedToken` wraps a\n",
      " |              string token to let you personalize its behavior: whether this token should only match against a single\n",
      " |              word, whether this token should strip all potential whitespaces on the left side, whether this token\n",
      " |              should strip all potential whitespaces on the right side, etc.\n",
      " |          special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
      " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
      " |      \n",
      " |              See details for :obj:`tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |          tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
      " |          model = BertModel.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |           # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |  \n",
      " |  sanitize_special_tokens(self) -> int\n",
      " |      Make sure that all the special tokens attributes of the tokenizer (:obj:`tokenizer.mask_token`,\n",
      " |      :obj:`tokenizer.cls_token`, etc.) are in the vocabulary.\n",
      " |      \n",
      " |      Add the missing ones to the vocabulary if needed.\n",
      " |      \n",
      " |      Return:\n",
      " |          :obj:`int`: The number of tokens added in the vocabulary during the operation.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      :obj:`List[int]`: List the ids of the special tokens(:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class\n",
      " |      attributes.\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      :obj:`List[str]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class attributes.\n",
      " |      \n",
      " |      Convert tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  all_special_tokens_extended\n",
      " |      :obj:`List[Union[str, tokenizers.AddedToken]]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.)\n",
      " |      mapped to class attributes.\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  pad_token_type_id\n",
      " |      :obj:`int`: Id of the padding token type in the vocabulary.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      :obj:`Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (:obj:`cls_token`,\n",
      " |      :obj:`unk_token`, etc.) to their values (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Convert potential tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  special_tokens_map_extended\n",
      " |      :obj:`Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary\n",
      " |      mapping special token class attributes (:obj:`cls_token`, :obj:`unk_token`, etc.) to their values\n",
      " |      (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  additional_special_tokens\n",
      " |      :obj:`List[str]`: All the additional special tokens you may want to use. Log an error if used while not having\n",
      " |      been set.\n",
      " |  \n",
      " |  additional_special_tokens_ids\n",
      " |      :obj:`List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not\n",
      " |      having been set.\n",
      " |  \n",
      " |  bos_token\n",
      " |      :obj:`str`: Beginning of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  bos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns :obj:`None` if the token\n",
      " |      has not been set.\n",
      " |  \n",
      " |  cls_token\n",
      " |      :obj:`str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the\n",
      " |      full depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token_id\n",
      " |      :obj:`Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input\n",
      " |      sequence leveraging self-attention along the full depth of the model.\n",
      " |      \n",
      " |      Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  eos_token\n",
      " |      :obj:`str`: End of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the end of sentence token in the vocabulary. Returns :obj:`None` if the token has\n",
      " |      not been set.\n",
      " |  \n",
      " |  mask_token\n",
      " |      :obj:`str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while\n",
      " |      not having been set.\n",
      " |  \n",
      " |  mask_token_id\n",
      " |      :obj:`Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n",
      " |      modeling. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  pad_token\n",
      " |      :obj:`str`: Padding token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_id\n",
      " |      :obj:`Optional[int]`: Id of the padding token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  sep_token\n",
      " |      :obj:`str`: Separation token, to separate context and query in an input sequence. Log an error if used while\n",
      " |      not having been set.\n",
      " |  \n",
      " |  sep_token_id\n",
      " |      :obj:`Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n",
      " |      sequence. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  unk_token\n",
      " |      :obj:`str`: Unknown token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  unk_token_id\n",
      " |      :obj:`Optional[int]`: Id of the unknown token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('transformers.PreTrainedTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_tkid(tokenizer, word):\n",
    "    tks = tokenizer.tokenize(word)\n",
    "    if len(tks) != 1:\n",
    "        raise Exception('word_to_tkid')\n",
    "    return tokenizer.convert_tokens_to_ids(tks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1285"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_tkid(tokenizer, 'Bonjour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'Bonjour', 'monde', '!', '</s>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(\"Bonjour monde !\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Bonjour monde!</s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Bonjour monde !\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'Ca', 'member', 't', '</s>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(\"Camembert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_confusion(vocab, filename):\n",
    "    rt = {}\n",
    "    for i in vocab.values():\n",
    "        rt[i] = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            l = line.split()\n",
    "            tk0 = int(l[0])\n",
    "            tk1 = int(l[1])\n",
    "            d = int(l[2])\n",
    "            rt[tk0].append((tk1, d))\n",
    "            rt[tk1].append((tk0, d))\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = load_confusion(tokenizer.get_vocab(), 'gen/confusion.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion(tokenizer, confusion, i):\n",
    "    c = [(d, j) for (j,d) in confusion[i]]\n",
    "    c.sort()\n",
    "    for (d,j) in c:\n",
    "        print(tokenizer.convert_ids_to_tokens(j), \":\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonjour : 1\n",
      "contour : 2\n",
      "Bonsoir : 2\n",
      "concour : 2\n",
      "Bonheur : 2\n",
      "conjur : 2\n"
     ]
    }
   ],
   "source": [
    "print_confusion(tokenizer, confusion, word_to_tkid(tokenizer, 'Bonjour'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correct(tokenizer, confusion, seq):\n",
    "    count = 0\n",
    "    for tk in seq:\n",
    "        if tk in confusion and confusion[tk]:\n",
    "            count += 1\n",
    "    count=torch.randint(count, [1]).item()\n",
    "    for (i,tk) in enumerate(seq):\n",
    "        if tk in confusion and confusion[tk]:\n",
    "            count -= 1\n",
    "            if count < 0:\n",
    "                masked_seq = seq[:]\n",
    "                masked_seq[i] = tokenizer.mask_token_id\n",
    "                return (i, tk, masked_seq)\n",
    "    raise Exception('make_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 164, '<s> Bonjour<mask>!</s>')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i,tk,seq = make_correct(tokenizer, confusion, tokenizer.encode('Bonjour monde !'))\n",
    "i,tk,tokenizer.decode(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_prob = [0, 1, 0.2]\n",
    "def make_error(tokenizer, confusion, seq):\n",
    "    s = 0\n",
    "    for tk in seq:\n",
    "        if tk not in confusion:\n",
    "            continue\n",
    "        for _,d in confusion[tk]:\n",
    "            s += confusion_prob[d]\n",
    "    s *= torch.rand(1).item()\n",
    "    for (i,tk) in enumerate(seq):\n",
    "        if tk not in confusion:\n",
    "            continue\n",
    "        for rep,d in confusion[tk]:\n",
    "            s -= confusion_prob[d]\n",
    "            if s < 0:\n",
    "                masked_seq = seq[:]\n",
    "                masked_seq[i] = tokenizer.mask_token_id\n",
    "                return (i, tk, rep, masked_seq)\n",
    "    raise Exception('make_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 83, 6777, '<s> Bonjour monde<mask></s>')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i,tk,rep,seq = make_error(tokenizer, confusion, tokenizer.encode('Bonjour monde !'))\n",
    "i,tk,rep,tokenizer.decode(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return [o[index[i]] for o in out]\n",
    "def index2(src, index):\n",
    "    flat_index = src.shape[1] * torch.tensor(range(src.shape[0])) + index\n",
    "    return torch.flatten(src,start_dim=0, end_dim=1)[flat_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  6, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11, 12]])\n",
    "trg = torch.tensor([0, 1, 1])\n",
    "index2(src, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedProb(nn.Module):\n",
    "    def __init__(self, transformer):\n",
    "        super(MaskedProb, self).__init__()\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def forward(self, index, tk, input_ids, attention_mask=None):\n",
    "        out = self.transformer(input_ids, attention_mask=attention_mask)[0]\n",
    "        out_index = index2(out, index)\n",
    "        out_token = index2(out_index, tk)\n",
    "        return out_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForMaskedLM were not initialized from the model checkpoint at camembert-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskedProb(\n",
       "  (transformer): CamembertForMaskedLM(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): RobertaLMHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=768, out_features=32005, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel = MaskedProb(CamembertForMaskedLM.from_pretrained(\"camembert-base\"))\n",
    "mmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correct_set(tokenizer, confusion, src_seqs, sample_per_seq=1):\n",
    "    indexes = []\n",
    "    tokens  = []\n",
    "    seqs    = []\n",
    "    for src_seq in src_seqs:\n",
    "        for _ in range(sample_per_seq):\n",
    "            i,tk,seq = make_correct(tokenizer, confusion, src_seq)\n",
    "            indexes.append(i)\n",
    "            tokens.append(tk)\n",
    "            seqs.append(seq)\n",
    "    return torch.tensor(indexes),torch.tensor(tokens),torch.tensor(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_error_set(tokenizer, confusion, src_seqs, sample_per_seq=1):\n",
    "    indexes = []\n",
    "    tokens  = []\n",
    "    seqs    = []\n",
    "    for src_seq in src_seqs:\n",
    "        for _ in range(sample_per_seq):\n",
    "            i,_,tk,seq = make_error(tokenizer, confusion, src_seq)\n",
    "            indexes.append(i)\n",
    "            tokens.append(tk)\n",
    "            seqs.append(seq)\n",
    "    return torch.tensor(indexes),torch.tensor(tokens),torch.tensor(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32004"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 1, 1, 1, 1]),\n",
       " tensor([ 164, 1285, 1285, 1285, 1285]),\n",
       " tensor([[    5,  1285, 32004,    83,     6],\n",
       "         [    5, 32004,   164,    83,     6],\n",
       "         [    5, 32004,   164,    83,     6],\n",
       "         [    5, 32004,   164,    83,     6],\n",
       "         [    5, 32004,   164,    83,     6]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_correct_set(tokenizer, confusion, [tokenizer.encode('Bonjour monde !')], sample_per_seq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.6110, 12.6879, 12.6879,  4.6110,  4.6110], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel(*make_correct_set(tokenizer, confusion, [tokenizer.encode('Bonjour monde !')], sample_per_seq=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8936,  2.0456,  4.7844,  3.7671,  3.6979], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel(*make_error_set(tokenizer, confusion, [tokenizer.encode('Bonjour monde !')], sample_per_seq=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset allocine_dataset (/home/ben/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/bbee2ebb45a067891973b91ebdd40a93598d1e2dd5710b6714cdc2cd81d0ed65)\n"
     ]
    }
   ],
   "source": [
    "src_dataset = load_dataset(\"allocine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'label'],\n",
       "        num_rows: 160000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review', 'label'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'label'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dataset = src_dataset['test'].select(range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'label'],\n",
       "    num_rows: 30\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ben/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/bbee2ebb45a067891973b91ebdd40a93598d1e2dd5710b6714cdc2cd81d0ed65/cache-09a92c40fb180311.arrow\n"
     ]
    }
   ],
   "source": [
    "tkn_dataset = src_dataset.map(\n",
    "    lambda x: tokenizer(x['review'], truncation=True, padding='max_length', max_length=50),\n",
    "    batched=True)\n",
    "del src_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'review'],\n",
       "    num_rows: 30\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_correct_data(src):\n",
    "    indexes = []\n",
    "    tokens  = []\n",
    "    seqs    = []\n",
    "    for src_seq in src['input_ids']:\n",
    "        i,tk,seq = make_correct(tokenizer, confusion, src_seq)\n",
    "        indexes.append(i)\n",
    "        tokens.append(tk)\n",
    "        seqs.append(seq)\n",
    "    return {'index': indexes, 'token':tokens, 'input_ids':seqs}\n",
    "\n",
    "def load_error_data(src):\n",
    "    indexes = []\n",
    "    tokens  = []\n",
    "    seqs    = []\n",
    "    for src_seq in src['input_ids']:\n",
    "        i,_,tk,seq = make_error(tokenizer, confusion, src_seq)\n",
    "        indexes.append(i)\n",
    "        tokens.append(tk)\n",
    "        seqs.append(seq)\n",
    "    return {'index': indexes, 'token':tokens, 'input_ids':seqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ben/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/bbee2ebb45a067891973b91ebdd40a93598d1e2dd5710b6714cdc2cd81d0ed65/cache-95df08c2333242f9.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bf3da506bd4ae8be302d7ebb63dc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_data = tkn_dataset.map(load_correct_data, batched=True, batch_size=1)\n",
    "error_data = tkn_dataset.map(load_error_data, batched=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'index', 'input_ids', 'label', 'review', 'token'],\n",
       "    num_rows: 30\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_data.set_format(type='torch', columns=['index', 'token', 'input_ids', 'attention_mask'])\n",
    "error_data.set_format(type='torch', columns=['index', 'token', 'input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_out(dataset):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "    outputs = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        out = mmodel(batch['index'], batch['token'], batch['input_ids'], batch['attention_mask'])\n",
    "        outputs.append(out)\n",
    "    return torch.cat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3dae0aae394701a707ea09fb04acd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08569cb6829144f0b84bcc8a36e53fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_out = compute_out(correct_data)\n",
    "error_out = compute_out(error_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12.9061, grad_fn=<MeanBackward0>),\n",
       " tensor(-0.3260, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_out.mean(), error_out.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved values\n",
    "correct_out = torch.tensor([ 8.0803, 14.0775, 20.3621, 20.1007,  1.8873, 16.6029, 17.8609, 20.0898,\n",
    "        20.7961,  4.7384, 14.6751, 13.5050, 21.4819, 12.9984,  3.6142,  3.6772,\n",
    "        15.0367, 17.9873,  9.6927, 17.9419,  4.8643, 14.4097, -1.7783, 16.8282,\n",
    "         5.7749, 13.0285, 10.2834, 18.6509, 22.5966,  7.3192])\n",
    "error_out = torch.tensor([ 2.7671, -5.2592,  3.5916,  1.6725,  4.0862, -3.8351, -6.8906, -0.1720,\n",
    "        -0.1972, -4.6426, -0.1949, -4.0336, -2.6897, -4.3467, -1.9231,  0.8093,\n",
    "         0.9162, -0.2468,  1.1669, -4.0763, 12.1963,  0.1935,  0.7147, -0.7231,\n",
    "         1.4321,  1.5579, -0.5683,  8.0869, -4.6107, -4.5620])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulated(vs):\n",
    "    xs = np.sort(vs)\n",
    "    ys = np.array(range(1,len(vs)+1)) / len(vs)\n",
    "    return xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ccx,ccy = cumulated(correct_out.detach().numpy())\n",
    "ecx,ecy = cumulated(error_out.detach().numpy())\n",
    "ccy = 100 * ccy       # false positive\n",
    "ecy = 100 * (1 - ecy) # false negative\n",
    "plt.plot(ccx, ccy, label='false positive')\n",
    "plt.plot(ecx, ecy, label='false negative')\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('%')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mask(masked_input, model, tokenizer, topk=5, input_word=None):\n",
    "    # Adapted from https://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/hub_interface.py\n",
    "    assert masked_input.count(\"<mask>\") == 1\n",
    "    input_ids = torch.tensor(tokenizer.encode(masked_input, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "    output = model(input_ids)\n",
    "    logits = output[0]  # The last hidden-state is the first element of the output tuple\n",
    "    masked_index = (input_ids.squeeze() == tokenizer.mask_token_id).nonzero(as_tuple=False).item()\n",
    "    logits = logits[0, masked_index, :]\n",
    "    prob = logits.softmax(dim=0)\n",
    "    \n",
    "    if input_word:\n",
    "        iw_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(input_word)[0])\n",
    "        print('iw logit: {:.3f}'.format(logits[iw_id]))\n",
    "    \n",
    "    values, indices = prob.topk(k=topk, dim=0)\n",
    "    topk_predicted_token_bpe = \" \".join(\n",
    "        [tokenizer.convert_ids_to_tokens(indices[i].item()) for i in range(len(indices))]\n",
    "    )\n",
    "    masked_token = tokenizer.mask_token\n",
    "    topk_filled_outputs = []\n",
    "    for index, predicted_token_bpe in enumerate(topk_predicted_token_bpe.split(\" \")):\n",
    "        predicted_token = predicted_token_bpe.replace(\"\\u2581\", \" \")\n",
    "        if \" {0}\".format(masked_token) in masked_input:\n",
    "            topk_filled_outputs.append(\n",
    "                (\n",
    "                    masked_input.replace(\" {0}\".format(masked_token), predicted_token),\n",
    "                    values[index].item(),\n",
    "                    predicted_token,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            topk_filled_outputs.append(\n",
    "                (masked_input.replace(masked_token, predicted_token), values[index].item(), predicted_token,)\n",
    "            )\n",
    "    return topk_filled_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iw logit: 5.593\n",
      "[('Le camembert est un', 0.3286605477333069, ' un'), ('Le camembert est ...', 0.182983860373497, ' ...'), ('Le camembert est une', 0.11833460628986359, ' une'), ('Le camembert est [...]', 0.10589182376861572, ' [...]'), ('Le camembert est le', 0.046711746603250504, ' le')]\n"
     ]
    }
   ],
   "source": [
    "masked_input = \"Le camembert est <mask>\"\n",
    "print(fill_mask(masked_input, model, tokenizer, input_word='servi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
